#+TITLE: Rate limiting bursty workloads
#+DATE: 2025-12-22
#+PROPERTY: header-args:sql :engine postgres :dbuser postgres :dbhost localhost :dbport 5432 :database postgres :cmdline "--quiet --no-psqlrc" :results value table

If a system is required to handle short bursts of requests from the
same users, the [[file:../rate-limiting.org][previous post]] strategy will not work. One solution in
this case is what is called a [[https://en.wikipedia.org/wiki/Token_bucket][Token Bucket]]. Each user starts with ~n~
tokens available, each requests consumes a token. Upon reaching 0
tokens, requests are denied. Tokens are replenished in a steady rate.

This post shows a implementation of rate limiting aimed at averaging
10 requests per second, with bursts of up to 10 requests (refilling at
a rate of 1 token per second).

#+NAME: init
#+BEGIN_SRC sql -n :results silent :tangle init.sql
create unlogged table rate_limit(
  id     int not null,
  tokens smallint not null default 9 check(tokens >= 0 and tokens <= 10),
  primary key (id)
);

create index on rate_limit
 using btree(tokens)
 where tokens < 10;
#+END_SRC

1: Unlogged table for faster writes;

[[file:tps-go-up.png]]

Please *do* change all your tables to unlogged without understanding
what you lose ==)=.

3: Inserting a new row occurs when a request is processed, ~tokens~
defaults to 9 (allowing 10 requests in total: the current one plus 9
remaining);

9: Partial indexing: keep (indexed) only rows that should be refilled.

Some SQL-fu, run for each request:
#+BEGIN_SRC sql -n :results silent :tangle insert.sql
\set id random(0, 10000)

with ins_or_upt as (
     insert into rate_limit as x (id)
     values (:id)
         on conflict(id) do
     update set tokens = x.tokens - 1
      where x.id = excluded.id
        and x.tokens > 0
  returning tokens
)
select tokens
  from ins_or_upt
 union all
select 0
 limit 1
#+END_SRC

1: The above snippet is used with [[https://www.postgresql.org/docs/current/pgbench.html][pgbench]] later, hence the use of
~\set~ command;

5: Using the variable set at line 1;

6: If a row already exists for ~:id~, update instead;

9: Returns the number of remaining ~tokens~, useful for the user;

11-15: Always return exactly one row containing the remaining ~tokens~
for ~:id~.

We can consume tokens, but they are never refilled (yet). The
system's backend could, of course, periodically issue a statement to
refill tokens one by one, /but that's not this blog's theme/. Enter
[[https://github.com/citusdata/pg_cron][pg_cron]], an extension that does just that: periodically executes a
statement. After installing it, we just schedule the following:

#+NAME: cron
#+BEGIN_SRC sql -n :results silent :tangle init.sql
select cron.schedule(
       '1 second',
       $$ update rate_limit
             set tokens = tokens + 1
           where tokens < 10 $$);
#+END_SRC

Done. This refills 1 token per second for rows with less than 10 tokens,
capping at 10 (full bucket after 10 seconds of inactivity). All of
this with no backend code.

Just for fun, I ran the insert script via pgbench using a single
connection, single client, single thread, simulating 10k concurrent
users.

#+NAME: pgbench
#+BEGIN_SRC sh -n :results verbatim :eval no-export
  docker exec -i pg pgbench \
         --time=30 \
         --client=1 \
         --jobs=1 \
         --protocol=prepared \
         --no-vacuum \
         --file=insert.sql
#+END_SRC

#+RESULTS: pgbench
#+begin_example
pgbench (18.1 (Debian 18.1-1.pgdg12+2))
transaction type: insert.sql
scaling factor: 1
query mode: prepared
number of clients: 1
number of threads: 1
maximum number of tries: 1
duration: 30 s
number of transactions actually processed: 628139
number of failed transactions: 0 (0.000%)
latency average = 0.048 ms
latency stddev = 0.189 ms
initial connection time = 1.982 ms
tps = 20939.329817 (without initial connection time)
#+end_example

You're right, that's not a proper benchmark. Still, achieving ~20k
transactions per second (tps) with my (okayish) setup and stock PostgreSQL
configuration makes me wonder why some (NoSQL) database systems
have such widespread adoption.

If you are wondering about how to run this yourself, download this files:
- [[file:Dockerfile][Dockerfile]]
- [[file:init.sql][init.sql]]
- [[file:insert.sql][insert.sql]]
- [[file:docker-compose.yml][docker-compose.yml]]

Place everything in the same directory and run: ~docker compose up~
and then execute pgbench (see [[pgbench]])

#+NAME: reset-docker
#+BEGIN_SRC sh :results silent :exports none :noweb yes
  docker compose down
  docker compose up --detach
#+END_SRC

#+NAME: reset
#+BEGIN_SRC sql :results silent :exports none :noweb yes
  drop extension if exists pg_cron;
  drop table if exists rate_limit;
  create extension pg_cron;
  <<init>>
  <<cron>>
#+END_SRC
