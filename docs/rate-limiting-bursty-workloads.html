<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Rate limiting bursty workloads</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="css/style.css" />
</head>
<body>
<div id="preamble" class="status">
<h1>Rate limiting bursty workloads</h1>
</div>
<div id="content" class="content">
<p>
If a system is required to handle short bursts of requests from the
same users, the <a href="rate-limiting.html">previous post</a> strategy will not work. One solution in
this case is what is called a <a href="https://en.wikipedia.org/wiki/Token_bucket">Token Bucket</a>. Each user starts with <code>n</code>
tokens available, each requests consumes a token. Upon reaching 0
tokens, requests are denied. Tokens are replenished in a steady rate.
</p>

<p>
This post shows a implementation of rate limiting aimed at averaging
10 requests per second, with bursts of up to 10 requests (refilling at
a rate of 1 token per second).
</p>

<div class="org-src-container">
<pre class="src src-sql" id="orgc1cb857"><span class="org-keyword">create</span> unlogged <span class="org-keyword">table</span> <span class="org-function-name">rate_limit</span>(       <span class="org-comment-delimiter">-- </span><span class="org-comment">1
</span>  id     <span class="org-type">int</span>      <span class="org-keyword">not</span> <span class="org-keyword">null</span>,
  tokens <span class="org-type">smallint</span> <span class="org-keyword">not</span> <span class="org-keyword">null</span>
                  <span class="org-keyword">default</span> 9             <span class="org-comment-delimiter">-- </span><span class="org-comment">2
</span>                  <span class="org-keyword">check</span>(tokens &gt;= 0 <span class="org-keyword">and</span>
                        tokens &lt;= 10),
  <span class="org-keyword">primary</span> <span class="org-keyword">key</span> (id)
);

<span class="org-keyword">create</span> index <span class="org-keyword">on</span> rate_limit
 <span class="org-keyword">using</span> btree(tokens)
 <span class="org-keyword">where</span> tokens &lt; 10;                     <span class="org-comment-delimiter">-- </span><span class="org-comment">3</span>
</pre>
</div>

<p>
1: Unlogged table for faster writes; Please <b>do</b> change all your
tables to unlogged without understanding what you lose =);
</p>

<p>
2: Inserting a new row occurs when a request is processed, <code>tokens</code>
defaults to 9 (allowing 10 requests in total: the current one plus 9
remaining);
</p>

<p>
3: Partial indexing: keep (indexed) only rows that should be refilled.
</p>

<p>
Some SQL-fu, run for each request:
</p>
<div class="org-src-container">
<pre class="src src-sql">\<span class="org-keyword">set</span> id random(0, 10000)                <span class="org-comment-delimiter">-- </span><span class="org-comment">1
</span>
<span class="org-keyword">with</span> ins_or_upt <span class="org-keyword">as</span>
(
     <span class="org-keyword">insert</span> <span class="org-keyword">into</span> rate_limit <span class="org-keyword">as</span> x (id)
     <span class="org-keyword">values</span> (:id)                       <span class="org-comment-delimiter">-- </span><span class="org-comment">2
</span>         <span class="org-keyword">on</span> conflict(id) do             <span class="org-comment-delimiter">-- </span><span class="org-comment">3
</span>     <span class="org-keyword">update</span> <span class="org-keyword">set</span> tokens = x.tokens - 1
      <span class="org-keyword">where</span> x.id = excluded.id
        <span class="org-keyword">and</span> x.tokens &gt; 0
  returning tokens                      <span class="org-comment-delimiter">-- </span><span class="org-comment">4
</span>)
<span class="org-keyword">select</span> tokens
  <span class="org-keyword">from</span> ins_or_upt
 <span class="org-keyword">union</span> <span class="org-keyword">all</span>                              <span class="org-comment-delimiter">-- </span><span class="org-comment">5
</span><span class="org-keyword">select</span> 0
 <span class="org-keyword">limit</span> 1
</pre>
</div>

<p>
1: The above snippet is used with <a href="https://www.postgresql.org/docs/current/pgbench.html">pgbench</a> later, hence the use of
<code>\set</code> command;
</p>

<p>
2: Using the variable set at line 1;
</p>

<p>
3: If a row already exists for <code>:id</code>, update instead;
</p>

<p>
4: Returns the number of remaining <code>tokens</code>, useful for the user;
</p>

<p>
5: Always return exactly one row containing the remaining <code>tokens</code> for
<code>:id</code>.
</p>

<p>
We can consume tokens, but they are never refilled (yet). The system's
backend could, of course, periodically issue a statement to refill
tokens one by one, <i>but that's not this blog's theme</i>. Enter <a href="https://github.com/citusdata/pg_cron">pg_cron</a>,
an extension that does just that: periodically executes a
statement. After installing it, we just schedule the following:
</p>

<div class="org-src-container">
<pre class="src src-sql" id="org4a34a4f"><span class="org-keyword">select</span> cron.schedule( <span class="org-string">'1 second'</span>
                    , $$ <span class="org-keyword">update</span> rate_limit
                            <span class="org-keyword">set</span> tokens = tokens + 1
                          <span class="org-keyword">where</span> tokens &lt; 10 $$ );
</pre>
</div>

<p>
Done. This refills 1 token per second for rows with less than 10
tokens, capping at 10 (full bucket after 10 seconds of
inactivity). All of this with no backend code.
</p>

<p>
Just for fun, I ran the insert script via pgbench using a single
connection, single client, single thread, simulating 10k concurrent
users.
</p>

<div class="org-src-container">
<pre class="src src-sh" id="org88f92b2">docker exec -i pg pgbench <span class="org-sh-escaped-newline">\</span>
       --time=30 <span class="org-sh-escaped-newline">\</span>
       --client=1 <span class="org-sh-escaped-newline">\</span>
       --jobs=1 <span class="org-sh-escaped-newline">\</span>
       --protocol=prepared <span class="org-sh-escaped-newline">\</span>
       --no-vacuum <span class="org-sh-escaped-newline">\</span>
       --file=insert.sql
</pre>
</div>

<p>
You're right, that's not a proper benchmark. Still, achieving ~20k
transactions per second (tps) with my (okayish) setup and stock
PostgreSQL configuration makes me wonder why some (NoSQL) database
systems have such widespread adoption.
</p>


<figure id="orgc9d378f">
<img src="rate-limiting-bursty-workloads/tps-go-up.avif" alt="tps-go-up.avif">

<figcaption><span class="figure-number">Figure 1: </span>me changing a table to unlogged</figcaption>
</figure>

<p>
If you are wondering about how to run this yourself, download this files:
</p>
<ul class="org-ul">
<li><a href="rate-limiting-bursty-workloads/postgres.Dockerfile">postgres.Dockerfile</a></li>
<li><a href="rate-limiting-bursty-workloads/init.sql">init.sql</a></li>
<li><a href="rate-limiting-bursty-workloads/insert.sql">insert.sql</a></li>
<li><a href="rate-limiting-bursty-workloads/docker-compose.yml">docker-compose.yml</a></li>
</ul>

<p>
Place everything in the same directory and run: <code>docker compose up</code>
and then execute pgbench.
</p>
</div>
<div id="postamble" class="status">
<p><small>2026-01-02</small></p>
</div>
</body>
</html>
